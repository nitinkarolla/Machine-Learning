
% Default to the notebook output style

    


% Inherit from the specified cell style.




    
\documentclass[11pt]{article}

    
    
    \usepackage[T1]{fontenc}
    % Nicer default font (+ math font) than Computer Modern for most use cases
    \usepackage{mathpazo}

    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % We will generate all images so they have a width \maxwidth. This means
    % that they will get their normal width if they fit onto the page, but
    % are scaled down if they would overflow the margins.
    \makeatletter
    \def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth
    \else\Gin@nat@width\fi}
    \makeatother
    \let\Oldincludegraphics\includegraphics
    % Set max figure width to be 80% of text width, for now hardcoded.
    \renewcommand{\includegraphics}[1]{\Oldincludegraphics[width=.8\maxwidth]{#1}}
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionLabelFormat{nolabel}{}
    \captionsetup{labelformat=nolabel}

    \usepackage{adjustbox} % Used to constrain images to a maximum size 
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro
    \usepackage[mathletters]{ucs} % Extended unicode (utf-8) support
    \usepackage[utf8x]{inputenc} % Allow utf-8 characters in the tex document
    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics 
                         % to support a larger range 
    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    

    
    
    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}
    
    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    
    
    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatability definitions
    \def\gt{>}
    \def\lt{<}
    % Document parameters
    \title{Decision Trees}
    
    
    

    % Pygments definitions
    
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\expandafter\def\csname PY@tok@w\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\expandafter\def\csname PY@tok@c\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.74,0.48,0.00}{##1}}}
\expandafter\def\csname PY@tok@k\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\expandafter\def\csname PY@tok@o\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ow\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@nb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@ne\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.82,0.25,0.23}{##1}}}
\expandafter\def\csname PY@tok@nv\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@no\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@nl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@ni\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.60,0.60,0.60}{##1}}}
\expandafter\def\csname PY@tok@na\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.49,0.56,0.16}{##1}}}
\expandafter\def\csname PY@tok@nt\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@s\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sd\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@si\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@se\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.13}{##1}}}
\expandafter\def\csname PY@tok@sr\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@ss\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sx\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@m\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@gh\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gu\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@gi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@gr\endcsname{\def\PY@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@ge\endcsname{\let\PY@it=\textit}
\expandafter\def\csname PY@tok@gs\endcsname{\let\PY@bf=\textbf}
\expandafter\def\csname PY@tok@gp\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@go\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.53,0.53}{##1}}}
\expandafter\def\csname PY@tok@gt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\expandafter\def\csname PY@tok@err\endcsname{\def\PY@bc##1{\setlength{\fboxsep}{0pt}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}
\expandafter\def\csname PY@tok@kc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kd\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kr\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@bp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@fm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@vc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vg\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sa\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@dl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s2\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s1\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@mb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@il\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mo\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ch\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cm\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cpf\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@c1\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cs\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % Exact colors from NB
    \definecolor{incolor}{rgb}{0.0, 0.0, 0.5}
    \definecolor{outcolor}{rgb}{0.545, 0.0, 0.0}



    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy 
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=linkcolor,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    
    

    \begin{document}
    
    
    \maketitle
    
    

    
    \section{CS 536 : Decision Trees}\label{cs-536-decision-trees}

\paragraph{Submitted by Nitin Reddy Karolla
(nrk60)}\label{submitted-by-nitin-reddy-karolla-nrk60}

    \subsubsection{1) For a given value of k, m, (number of features, number
of data points), write a function to generate a training data set based
on the above
scheme.}\label{for-a-given-value-of-k-m-number-of-features-number-of-data-points-write-a-function-to-generate-a-training-data-set-based-on-the-above-scheme.}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}3}]:} \PY{c+c1}{\PYZsh{} Importing the required packages for the assignment}
        
        \PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}
        \PY{k+kn}{import} \PY{n+nn}{math}
        \PY{k+kn}{import} \PY{n+nn}{pandas} \PY{k}{as} \PY{n+nn}{pd}
        \PY{k+kn}{import} \PY{n+nn}{seaborn} \PY{k}{as} \PY{n+nn}{sns}
        \PY{k+kn}{import} \PY{n+nn}{matplotlib}\PY{n+nn}{.}\PY{n+nn}{pyplot} \PY{k}{as} \PY{n+nn}{plt}
        \PY{k+kn}{from} \PY{n+nn}{tqdm} \PY{k}{import} \PY{n}{tqdm}
        
        \PY{o}{\PYZpc{}}\PY{k}{matplotlib} inline
\end{Verbatim}


    Following function generates the data as mentioned in question. Function
expect m and k where m is the number of samples and k is the number of
features. The data is generated based on the scheme mentioned in the
question.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}4}]:} \PY{k}{def} \PY{n+nf}{data\PYZus{}generator}\PY{p}{(}\PY{n}{m}\PY{p}{,}\PY{n}{k}\PY{p}{)}\PY{p}{:}
            \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{l+s+sd}{    Following function generates data required for the decision trees.}
        \PY{l+s+sd}{    Scheme : }
        \PY{l+s+sd}{        X1 = 1 with probability 1/2, X1 = 0 with probability 1/2}
        \PY{l+s+sd}{        For i = 2, . . . , k, Xi = Xi−1 with probability 3/4, and }
        \PY{l+s+sd}{        Xi = 1 − Xi−1 with probability 1/4}
        \PY{l+s+sd}{        Y = X1 if w2X2 + w3X3 + . . . + wkXk ≥ 1/2}
        \PY{l+s+sd}{            or 1 − X1 else}
        \PY{l+s+sd}{    Input : }
        \PY{l+s+sd}{        m \PYZhy{} Number of samples}
        \PY{l+s+sd}{        k \PYZhy{} Number of features}
        \PY{l+s+sd}{    }
        \PY{l+s+sd}{    Output : }
        \PY{l+s+sd}{        Dataframe with k+1 columns i.e. k features that represent X }
        \PY{l+s+sd}{        and 1 the represents Y}
        \PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
            \PY{n}{X} \PY{o}{=} \PY{p}{[}\PY{p}{]} 
            \PY{k}{for} \PY{n}{j} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{m}\PY{p}{)}\PY{p}{:}
                \PY{n}{y} \PY{o}{=} \PY{p}{[}\PY{p}{]}
                \PY{n}{a} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{binomial}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{p}\PY{o}{=}\PY{l+m+mf}{0.5}\PY{p}{,} \PY{n}{size}\PY{o}{=} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
                \PY{n}{y}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{a}\PY{p}{)}
                \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{k}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{:}
                    \PY{n}{x} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{binomial}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{p}\PY{o}{=}\PY{l+m+mf}{0.75}\PY{p}{,} \PY{n}{size}\PY{o}{=} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
                    \PY{k}{if} \PY{n}{x} \PY{o}{==} \PY{l+m+mi}{1}\PY{p}{:}
                        \PY{n}{y}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{a}\PY{p}{)}
                    \PY{k}{else}\PY{p}{:}
                        \PY{n}{y}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{l+m+mi}{1}\PY{o}{\PYZhy{}}\PY{n}{a}\PY{p}{)}
                        \PY{n}{a} \PY{o}{=} \PY{l+m+mi}{1}\PY{o}{\PYZhy{}}\PY{n}{a}
                \PY{n}{X}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{y}\PY{p}{)}
                
            \PY{c+c1}{\PYZsh{}Generating the Weights}
            \PY{n}{j} \PY{o}{=} \PY{l+m+mf}{0.9}\PY{o}{*}\PY{l+m+mf}{0.9}
            \PY{n}{s} \PY{o}{=} \PY{n}{j} \PY{o}{*} \PY{p}{(}\PY{l+m+mi}{1} \PY{o}{\PYZhy{}} \PY{p}{(}\PY{l+m+mf}{0.9} \PY{o}{*}\PY{o}{*} \PY{p}{(}\PY{n}{k}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}\PY{p}{)}\PY{o}{/} \PY{p}{(}\PY{l+m+mi}{1}\PY{o}{\PYZhy{}} \PY{l+m+mf}{0.9}\PY{p}{)}
            \PY{n}{w} \PY{o}{=} \PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
            \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{k}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{:}
                \PY{n}{w\PYZus{}i} \PY{o}{=} \PY{l+m+mf}{0.9}\PY{o}{*}\PY{o}{*}\PY{p}{(}\PY{n}{i}\PY{o}{+}\PY{l+m+mi}{2}\PY{p}{)}\PY{o}{/} \PY{n}{s}
                \PY{n}{w}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{w\PYZus{}i}\PY{p}{)}
            
            \PY{c+c1}{\PYZsh{}Generating the Y}
            \PY{n}{Y} \PY{o}{=} \PY{p}{[}\PY{p}{]}
            \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{m}\PY{p}{)}\PY{p}{:}
                \PY{n}{y\PYZus{}i} \PY{o}{=} \PY{p}{[}\PY{n}{a}\PY{o}{*}\PY{n}{b} \PY{k}{for} \PY{n}{a}\PY{p}{,}\PY{n}{b} \PY{o+ow}{in} \PY{n+nb}{zip}\PY{p}{(}\PY{n}{X}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{,}\PY{n}{w}\PY{p}{)}\PY{p}{]}
                \PY{k}{if} \PY{n+nb}{sum}\PY{p}{(}\PY{n}{y\PYZus{}i}\PY{p}{)} \PY{o}{\PYZgt{}}\PY{o}{=} \PY{l+m+mf}{0.5}\PY{p}{:}
                    \PY{n}{Y}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{X}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}
                \PY{k}{else}\PY{p}{:}
                    \PY{n}{Y}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{l+m+mi}{1}\PY{o}{\PYZhy{}}\PY{n}{X}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}
            
            \PY{c+c1}{\PYZsh{}Generate column names}
            \PY{n}{colnames} \PY{o}{=} \PY{p}{[} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{X}\PY{l+s+s1}{\PYZsq{}}\PY{o}{+} \PY{n+nb}{str}\PY{p}{(}\PY{n}{i}\PY{p}{)}  \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,}\PY{n}{k}\PY{o}{+}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{]}
            
            \PY{n}{df} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{data}\PY{o}{=} \PY{n}{X}\PY{p}{,} \PY{n}{columns}\PY{o}{=} \PY{n}{colnames}\PY{p}{)}
            \PY{n}{df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Y}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{Series}\PY{p}{(}\PY{n}{data} \PY{o}{=} \PY{n}{Y}\PY{p}{)}
            \PY{k}{return}\PY{p}{(}\PY{n}{df}\PY{p}{)}
\end{Verbatim}


    \subsubsection{2) Given a data set, write a function to fit a decision
tree to that data based on splitting the variables by maximizing the
information gain. Additionally, return the training error of this tree
on the data set, err\_train(ˆf). It may be useful to have a function
that takes a data set and a variable, and returns the data set
partitioned based on the values of that
variable.}\label{given-a-data-set-write-a-function-to-fit-a-decision-tree-to-that-data-based-on-splitting-the-variables-by-maximizing-the-information-gain.-additionally-return-the-training-error-of-this-tree-on-the-data-set-err_trainux2c6f.-it-may-be-useful-to-have-a-function-that-takes-a-data-set-and-a-variable-and-returns-the-data-set-partitioned-based-on-the-values-of-that-variable.}

    Let us create data structure called Node which will be holding on
information like Data, Rule and the Children at that node in a decision
tree.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}5}]:} \PY{k}{class} \PY{n+nc}{Node}\PY{p}{(}\PY{p}{)}\PY{p}{:}
            \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{l+s+sd}{    Node is a data structure which will be used for decision trees.}
        \PY{l+s+sd}{    }
        \PY{l+s+sd}{    Input :}
        \PY{l+s+sd}{        data = training data post split is stored}
        \PY{l+s+sd}{        rule = feature on which the split led to this node and the }
        \PY{l+s+sd}{        value of the feature}
        \PY{l+s+sd}{        child = nodes of children of all this node are present }
        \PY{l+s+sd}{        after the split}
        \PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
            \PY{k}{def} \PY{n+nf}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,}
                         \PY{n}{data} \PY{o}{=} \PY{k+kc}{None}\PY{p}{,}
                         \PY{n}{rule} \PY{o}{=} \PY{k+kc}{None}\PY{p}{,}
                         \PY{n}{child} \PY{o}{=} \PY{k+kc}{None}
                        \PY{p}{)}\PY{p}{:}
                \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{data} \PY{o}{=} \PY{n}{data}
                \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{rule} \PY{o}{=} \PY{n}{rule}
                \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{child} \PY{o}{=} \PY{n}{child}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}6}]:} \PY{k}{class} \PY{n+nc}{Decision\PYZus{}Tree\PYZus{}ID3}\PY{p}{(}\PY{p}{)}\PY{p}{:}
            \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{l+s+sd}{    Decision Tree ID3 is trained on data with a target variable. It }
        \PY{l+s+sd}{    is built on split variable which is indentified using the logic }
        \PY{l+s+sd}{    of information gain }
        \PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
            \PY{k}{def} \PY{n+nf}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{root} \PY{o}{=} \PY{k+kc}{None}\PY{p}{)}\PY{p}{:}
                \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{root} \PY{o}{=} \PY{n}{root}
                
                
            \PY{k}{def} \PY{n+nf}{entropy}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{data}\PY{p}{,} \PY{n}{variable}\PY{p}{)}\PY{p}{:}
                \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{l+s+sd}{        Calcuates the entropy for the given data and target variable}
        \PY{l+s+sd}{        \PYZdq{}\PYZdq{}\PYZdq{}}
                \PY{n}{entropy\PYZus{}value} \PY{o}{=} \PY{n+nb}{sum}\PY{p}{(}\PY{p}{[}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{n}{data}\PY{p}{[}\PY{n}{variable}\PY{p}{]}\PY{o}{.}\PY{n}{value\PYZus{}counts}\PY{p}{(}\PY{p}{)}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{o}{/} 
                                      \PY{n}{data}\PY{p}{[}\PY{n}{variable}\PY{p}{]}\PY{o}{.}\PY{n}{count}\PY{p}{(}\PY{p}{)}\PY{p}{)} \PY{o}{*} \PY{n}{np}\PY{o}{.}\PY{n}{log2}\PY{p}{(}\PY{p}{(}
                    \PY{n}{data}\PY{p}{[}\PY{n}{variable}\PY{p}{]}\PY{o}{.}\PY{n}{value\PYZus{}counts}\PY{p}{(}\PY{p}{)}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{o}{/} 
                    \PY{n}{data}\PY{p}{[}\PY{n}{variable}\PY{p}{]}\PY{o}{.}\PY{n}{count}\PY{p}{(}\PY{p}{)}\PY{p}{)} \PY{o}{+} \PY{l+m+mf}{0.00000001}\PY{p}{)} 
                                     \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n}{data}\PY{p}{[}\PY{n}{variable}\PY{p}{]}\PY{o}{.}\PY{n}{unique}\PY{p}{(}\PY{p}{)}\PY{p}{]}\PY{p}{)}
                \PY{k}{return} \PY{n}{entropy\PYZus{}value}
            
            
            \PY{k}{def} \PY{n+nf}{information\PYZus{}gain}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{data}\PY{p}{,} \PY{n}{variable}\PY{p}{,} \PY{n}{target}\PY{p}{)}\PY{p}{:}
                \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{l+s+sd}{        Calculates the information gain for the given variable and data}
        \PY{l+s+sd}{        \PYZdq{}\PYZdq{}\PYZdq{}}
                \PY{n}{infomation\PYZus{}content} \PY{o}{=} \PY{n+nb}{sum}\PY{p}{(}\PY{p}{[}\PY{n}{data}\PY{p}{[}\PY{n}{variable}\PY{p}{]}\PY{o}{.}\PY{n}{value\PYZus{}counts}\PY{p}{(}\PY{p}{)}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{o}{/}
                                          \PY{n}{data}\PY{p}{[}\PY{n}{variable}\PY{p}{]}\PY{o}{.}\PY{n}{count}\PY{p}{(}\PY{p}{)}
                                          \PY{o}{*} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{entropy}\PY{p}{(}\PY{n}{data}\PY{p}{[}\PY{n}{data}\PY{p}{[}\PY{n}{variable}\PY{p}{]}\PY{o}{==} \PY{n}{i}\PY{p}{]}\PY{p}{,} 
                                                         \PY{n}{target}\PY{p}{)} 
                                          \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n}{data}\PY{p}{[}\PY{n}{variable}\PY{p}{]}\PY{o}{.}\PY{n}{unique}\PY{p}{(}\PY{p}{)}\PY{p}{]}\PY{p}{)}
                \PY{n}{info\PYZus{}gain} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{entropy}\PY{p}{(}\PY{n}{data}\PY{p}{,} \PY{n}{target}\PY{p}{)} \PY{o}{\PYZhy{}} \PY{n}{infomation\PYZus{}content}
                \PY{k}{return}\PY{p}{(}\PY{n}{info\PYZus{}gain}\PY{p}{)}
            
            
            \PY{k}{def} \PY{n+nf}{split\PYZus{}variable\PYZus{}identification}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{data}\PY{p}{,} \PY{n}{target}\PY{p}{)}\PY{p}{:}
                \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{l+s+sd}{        Identifies the split variable based on data and target}
        \PY{l+s+sd}{        \PYZdq{}\PYZdq{}\PYZdq{}}
                \PY{c+c1}{\PYZsh{}loop through all features and calculate information gain for each feature}
                \PY{n}{variable\PYZus{}ig\PYZus{}required} \PY{o}{=} \PY{n+nb}{list}\PY{p}{(}\PY{n}{data}\PY{o}{.}\PY{n}{columns}\PY{p}{)}
                \PY{n}{variable\PYZus{}ig\PYZus{}required}\PY{o}{.}\PY{n}{remove}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Y}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
                \PY{n}{ig\PYZus{}values} \PY{o}{=} \PY{p}{[}\PY{p}{(}\PY{n}{i}\PY{p}{,} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{information\PYZus{}gain}\PY{p}{(}\PY{n}{data}\PY{p}{,}\PY{n}{i}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Y}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)} 
                             \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n}{variable\PYZus{}ig\PYZus{}required}\PY{p}{]}
                \PY{k}{if} \PY{n+nb}{len}\PY{p}{(}\PY{n}{ig\PYZus{}values}\PY{p}{)} \PY{o}{!=} \PY{l+m+mi}{0}\PY{p}{:}
                    \PY{n}{split\PYZus{}variable} \PY{o}{=} \PY{n+nb}{max}\PY{p}{(}\PY{n}{ig\PYZus{}values}\PY{p}{,} \PY{n}{key} \PY{o}{=} \PY{k}{lambda} \PY{n}{item} \PY{p}{:} \PY{n}{item}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}
                \PY{k}{else}\PY{p}{:}
                    \PY{n}{split\PYZus{}variable} \PY{o}{=} \PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{)}
                \PY{k}{return}\PY{p}{(}\PY{n}{split\PYZus{}variable}\PY{p}{)}
            
        
            \PY{k}{def} \PY{n+nf}{split\PYZus{}data}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{data}\PY{p}{,} \PY{n}{split\PYZus{}variable}\PY{p}{)}\PY{p}{:} 
                \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{l+s+sd}{        Splits the data after identifying the split variable, assigns }
        \PY{l+s+sd}{        data and rule to the node.}
        \PY{l+s+sd}{        \PYZdq{}\PYZdq{}\PYZdq{}}
                \PY{n}{splitted\PYZus{}data} \PY{o}{=} \PY{p}{[}\PY{n}{Node}\PY{p}{(}\PY{n}{data} \PY{o}{=} \PY{p}{(}\PY{n}{data}\PY{p}{[}\PY{n}{data}\PY{p}{[}\PY{n}{split\PYZus{}variable}\PY{p}{]} \PY{o}{==} \PY{n}{i}\PY{p}{]}\PY{o}{.}
                                              \PY{n}{drop}\PY{p}{(}\PY{n}{split\PYZus{}variable}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}\PY{p}{,}
                                      \PY{n}{rule} \PY{o}{=} \PY{p}{(}\PY{n}{split\PYZus{}variable}\PY{p}{,}\PY{n}{i}\PY{p}{)}\PY{p}{)} 
                                 \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n}{data}\PY{p}{[}\PY{n}{split\PYZus{}variable}\PY{p}{]}\PY{o}{.}\PY{n}{unique}\PY{p}{(}\PY{p}{)}\PY{p}{]}
                \PY{k}{return}\PY{p}{(}\PY{n}{splitted\PYZus{}data}\PY{p}{)}
            
            
            \PY{k}{def} \PY{n+nf}{fit}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{data}\PY{p}{,} \PY{n}{target}\PY{p}{)}\PY{p}{:}
                \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{l+s+sd}{        Fit is used to fit decision trees on the data for a given target variable}
        \PY{l+s+sd}{        \PYZdq{}\PYZdq{}\PYZdq{}}
                \PY{k}{if} \PY{n+nb}{type}\PY{p}{(}\PY{n}{data}\PY{p}{)} \PY{o}{!=} \PY{n}{Node}\PY{p}{:}
                    \PY{n}{data} \PY{o}{=} \PY{n}{Node}\PY{p}{(}\PY{n}{data} \PY{o}{=} \PY{n}{data}\PY{p}{)}
                    \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{root} \PY{o}{=} \PY{n}{data}
        
                \PY{k}{if} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{split\PYZus{}variable\PYZus{}identification}\PY{p}{(}\PY{n}{data}\PY{o}{.}\PY{n}{data}\PY{p}{,} \PY{n}{target}\PY{p}{)}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]} \PY{o}{==} \PY{l+m+mi}{0}\PY{p}{:}
                    \PY{k}{return}
        
                \PY{n}{split\PYZus{}variable} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{split\PYZus{}variable\PYZus{}identification}\PY{p}{(}\PY{n}{data}\PY{o}{.}\PY{n}{data}\PY{p}{,} \PY{n}{target}\PY{p}{)}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
                \PY{n}{data}\PY{o}{.}\PY{n}{child} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{split\PYZus{}data}\PY{p}{(}\PY{n}{data}\PY{o}{.}\PY{n}{data}\PY{p}{,} \PY{n}{split\PYZus{}variable}\PY{p}{)}
        
                \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n}{data}\PY{o}{.}\PY{n}{child}\PY{p}{:}
                    \PY{k}{if} \PY{n}{i}\PY{o}{.}\PY{n}{data}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Y}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{nunique}\PY{p}{(}\PY{p}{)} \PY{o}{!=} \PY{l+m+mi}{1}\PY{p}{:} 
                        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{i}\PY{p}{,} \PY{n}{target}\PY{p}{)}
                    
            
            \PY{k}{def} \PY{n+nf}{get\PYZus{}rules}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{model} \PY{o}{=} \PY{k+kc}{None} \PY{p}{,}\PY{n}{ruleList} \PY{o}{=} \PY{p}{[}\PY{p}{]}\PY{p}{)}\PY{p}{:}
                \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{l+s+sd}{        Returns the rules for each leaf and the major class in the leaf}
        \PY{l+s+sd}{        \PYZdq{}\PYZdq{}\PYZdq{}}
                \PY{k}{if} \PY{n}{model} \PY{o}{==} \PY{k+kc}{None}\PY{p}{:}
                    \PY{n}{model} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{root}
                \PY{n}{ruleList}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{model}\PY{o}{.}\PY{n}{rule}\PY{p}{)}
                \PY{k}{if} \PY{n}{model}\PY{o}{.}\PY{n}{child} \PY{o}{==} \PY{k+kc}{None}\PY{p}{:}
                    \PY{n}{ruleList}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{model}\PY{o}{.}\PY{n}{data}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Y}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{mode}\PY{p}{(}\PY{p}{)}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}
                    \PY{k}{return} \PY{n+nb}{print}\PY{p}{(}\PY{n}{ruleList}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{:}\PY{p}{]}\PY{p}{)}
                \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n}{model}\PY{o}{.}\PY{n}{child}\PY{p}{:}
                    \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{get\PYZus{}rules}\PY{p}{(}\PY{n}{i}\PY{p}{,}\PY{n}{ruleList}\PY{o}{.}\PY{n}{copy}\PY{p}{(}\PY{p}{)}\PY{p}{)}
            
        
            \PY{k}{def} \PY{n+nf}{predict\PYZus{}row}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{model}\PY{p}{,} \PY{n}{row}\PY{p}{)}\PY{p}{:}
                \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{l+s+sd}{        This function returns the prediction for the a single sample of }
        \PY{l+s+sd}{        data using the fitted data}
        \PY{l+s+sd}{        \PYZdq{}\PYZdq{}\PYZdq{}}
                \PY{k}{if} \PY{n}{model}\PY{o}{.}\PY{n}{child} \PY{o}{==} \PY{k+kc}{None}\PY{p}{:}
                    \PY{k}{return}\PY{p}{(}\PY{n}{model}\PY{o}{.}\PY{n}{data}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Y}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{mode}\PY{p}{(}\PY{p}{)}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}
        
                \PY{n}{variable} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{child}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{rule}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
                \PY{n}{row\PYZus{}value} \PY{o}{=} \PY{n}{row}\PY{p}{[}\PY{n}{variable}\PY{p}{]}
                \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n}{model}\PY{o}{.}\PY{n}{child}\PY{p}{:}
                    \PY{k}{if} \PY{n}{i}\PY{o}{.}\PY{n}{rule}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]} \PY{o}{==} \PY{n}{row\PYZus{}value}\PY{p}{:}
                        \PY{k}{return} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{predict\PYZus{}row}\PY{p}{(}\PY{n}{i}\PY{p}{,} \PY{n}{row}\PY{p}{)}
                    
            \PY{k}{def} \PY{n+nf}{predict}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{test}\PY{p}{)}\PY{p}{:}
                \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{l+s+sd}{        Predict funtion will take an input data and return the prediction}
        \PY{l+s+sd}{        based on the fitted decision tree}
        \PY{l+s+sd}{        \PYZdq{}\PYZdq{}\PYZdq{}}
                \PY{n}{predicted\PYZus{}y} \PY{o}{=} \PY{p}{[}\PY{p}{]}
                \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n}{test}\PY{o}{.}\PY{n}{iterrows}\PY{p}{(}\PY{p}{)}\PY{p}{:}
                    \PY{n}{x} \PY{o}{=} \PY{n}{i}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}
                    \PY{n}{y} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{predict\PYZus{}row}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{root}\PY{p}{,} \PY{n}{x}\PY{p}{)}
                    \PY{n}{predicted\PYZus{}y}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{y}\PY{p}{)}
                \PY{k}{return} \PY{n}{pd}\PY{o}{.}\PY{n}{Series}\PY{p}{(}\PY{n}{predicted\PYZus{}y}\PY{p}{)}
            
            \PY{k}{def} \PY{n+nf}{training\PYZus{}error}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{)}\PY{p}{:}
                \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{l+s+sd}{        Returns the training error of the  fitted decision tree}
        \PY{l+s+sd}{        \PYZdq{}\PYZdq{}\PYZdq{}}
                \PY{n}{predict\PYZus{}train} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{data}\PY{p}{)}
                \PY{k}{return} \PY{p}{(}\PY{l+m+mi}{1} \PY{o}{\PYZhy{}}\PY{n+nb}{sum}\PY{p}{(}\PY{n}{data}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Y}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{==} \PY{n}{predict\PYZus{}train}\PY{p}{)}\PY{o}{/} \PY{n+nb}{len}\PY{p}{(}\PY{n}{data}\PY{p}{)}\PY{p}{)}
                    
\end{Verbatim}


    \subsubsection{3) For k = 4 and m = 30, generate data and fit a decision
tree to it. Does the ordering of the variables in the decision tree make
sense, based on the function that defines Y ? Why or why not? Draw the
tree.}\label{for-k-4-and-m-30-generate-data-and-fit-a-decision-tree-to-it.-does-the-ordering-of-the-variables-in-the-decision-tree-make-sense-based-on-the-function-that-defines-y-why-or-why-not-draw-the-tree.}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}5}]:} \PY{c+c1}{\PYZsh{}Generating the data for m = 30 , k = 4}
        \PY{n}{data} \PY{o}{=} \PY{n}{data\PYZus{}generator}\PY{p}{(}\PY{n}{m} \PY{o}{=}\PY{l+m+mi}{30}\PY{p}{,} \PY{n}{k} \PY{o}{=}\PY{l+m+mi}{4}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}6}]:} \PY{n}{model} \PY{o}{=} \PY{n}{Decision\PYZus{}Tree\PYZus{}ID3}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}7}]:} \PY{n}{model}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{data}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Y}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}8}]:} \PY{n}{model}\PY{o}{.}\PY{n}{get\PYZus{}rules}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
[('X2', 0), ('X4', 1), ('X3', 1), 0]
[('X2', 0), ('X4', 1), ('X3', 0), 1]
[('X2', 0), ('X4', 0), 1]
[('X2', 1), ('X1', 1), ('X3', 1), 1]
[('X2', 1), ('X1', 1), ('X3', 0), ('X4', 0), 0]
[('X2', 1), ('X1', 1), ('X3', 0), ('X4', 1), 1]
[('X2', 1), ('X1', 0), 0]

    \end{Verbatim}

    Above is the decision tree, i.e. it gives the 'Y' value which is last
element of each array based on the various values of features. For
Example , X4 =0, X1 =1, X3 = 1, X2=1 will have "Y" value has 1.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}9}]:} \PY{n}{model}\PY{o}{.}\PY{n}{training\PYZus{}error}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}9}]:} 0.0
\end{Verbatim}
            
    As observed in various decision trees, 'X1' feature seems to play a
major roles among the ordering in which the split variables is chosen,
and it makes sense as X1 plays an important role in determing the value
of Y. Also, X1 interaction with other variables plays important role and
mostly the initital features as weight being multiplied is higher and
increases the chance of sum being \textgreater{} 1/2 which inturn plays
a role in determining the Y.

Also, we notice that Y is deterministic with respect to the values of X.

    \subsubsection{4) Write a function that takes a decision tree and
estimates its typical error on this data err( ˆf); i.e., generate a lot
of data according to the above scheme, and find the average error rate
of this tree over that
data}\label{write-a-function-that-takes-a-decision-tree-and-estimates-its-typical-error-on-this-data-err-ux2c6f-i.e.-generate-a-lot-of-data-according-to-the-above-scheme-and-find-the-average-error-rate-of-this-tree-over-that-data}

    For the same decision tree built above, lets find the average error rate
of this tree.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}13}]:} \PY{k}{def} \PY{n+nf}{typical\PYZus{}error}\PY{p}{(}\PY{n}{model}\PY{p}{,} \PY{n}{test\PYZus{}data\PYZus{}size}\PY{p}{,} \PY{n}{simulations} \PY{o}{=} \PY{l+m+mi}{50}\PY{p}{)}\PY{p}{:}
             \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
         \PY{l+s+sd}{    Generate error for similations for a generated test data and }
         \PY{l+s+sd}{    given decision tree.}
         \PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
             \PY{n}{k} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{root}\PY{o}{.}\PY{n}{data}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}
             \PY{n}{error} \PY{o}{=} \PY{p}{[}\PY{p}{]}
             \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{simulations}\PY{p}{)}\PY{p}{:}
                 \PY{n}{test\PYZus{}data} \PY{o}{=} \PY{n}{data\PYZus{}generator}\PY{p}{(}\PY{n}{test\PYZus{}data\PYZus{}size}\PY{p}{,} \PY{n}{k}\PY{p}{)}
                 \PY{n}{predicted\PYZus{}y} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{test\PYZus{}data}\PY{p}{)}
                 \PY{n}{error\PYZus{}current} \PY{o}{=} \PY{l+m+mi}{1} \PY{o}{\PYZhy{}}\PY{p}{(}\PY{n+nb}{sum}\PY{p}{(}\PY{n}{test\PYZus{}data}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Y}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{==} \PY{n}{predicted\PYZus{}y}\PY{p}{)}\PY{o}{/} \PY{n+nb}{len}\PY{p}{(}\PY{n}{test\PYZus{}data}\PY{p}{)}\PY{p}{)}
                 \PY{n}{error}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{error\PYZus{}current}\PY{p}{)}
             \PY{k}{return} \PY{n}{error}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}11}]:} \PY{n}{err\PYZus{}f} \PY{o}{=} \PY{n}{typical\PYZus{}error}\PY{p}{(}\PY{n}{model}\PY{p}{,} \PY{n}{test\PYZus{}data\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{30}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}12}]:} \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Average typical error of the decision tree is }\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n+nb}{sum}\PY{p}{(}\PY{n}{err\PYZus{}f}\PY{p}{)}\PY{o}{/}\PY{n+nb}{len}\PY{p}{(}\PY{n}{err\PYZus{}f}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Average typical error of the decision tree is  0.14333333333333328

    \end{Verbatim}

    \subsubsection{5) For k = 10, estimate the value of
\textbar{}errtrain(ˆf) − err( ˆf)\textbar{} for a given m by repeatedly
generating data sets, fitting trees to those data sets, and estimating
the true and training error. Do this for multiple m, and graph this
difference as a function of m. What can you say about the marginal value
of additional training
data?}\label{for-k-10-estimate-the-value-of-errtrainux2c6f-err-ux2c6f-for-a-given-m-by-repeatedly-generating-data-sets-fitting-trees-to-those-data-sets-and-estimating-the-true-and-training-error.-do-this-for-multiple-m-and-graph-this-difference-as-a-function-of-m.-what-can-you-say-about-the-marginal-value-of-additional-training-data}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}13}]:} \PY{c+c1}{\PYZsh{}Let us define the values required to understand the marginal value of additional training data}
         \PY{n}{k} \PY{o}{=} \PY{l+m+mi}{10}
         
         \PY{c+c1}{\PYZsh{}Defining the repetitions values for generating m samples of training data}
         \PY{n}{start} \PY{o}{=} \PY{l+m+mi}{50}
         \PY{n}{end} \PY{o}{=} \PY{l+m+mi}{2500}
         \PY{n}{step} \PY{o}{=} \PY{l+m+mi}{25}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}14}]:} \PY{n}{err\PYZus{}abs} \PY{o}{=} \PY{p}{[}\PY{p}{]}
         \PY{n}{m} \PY{o}{=} \PY{p}{[}\PY{p}{]}
         \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n}{tqdm}\PY{p}{(}\PY{n+nb}{range}\PY{p}{(}\PY{n}{start}\PY{p}{,}\PY{n}{end}\PY{p}{,}\PY{n}{step}\PY{p}{)}\PY{p}{)}\PY{p}{:}
             \PY{n}{data} \PY{o}{=} \PY{n}{data\PYZus{}generator}\PY{p}{(}\PY{n}{i}\PY{p}{,} \PY{n}{k} \PY{o}{=} \PY{l+m+mi}{10}\PY{p}{)}
             \PY{n}{dt} \PY{o}{=} \PY{n}{Decision\PYZus{}Tree\PYZus{}ID3}\PY{p}{(}\PY{p}{)}
             \PY{n}{dt}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{data}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Y}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
             \PY{n}{err\PYZus{}f} \PY{o}{=} \PY{n}{typical\PYZus{}error}\PY{p}{(}\PY{n}{dt}\PY{p}{,} \PY{l+m+mi}{100}\PY{p}{)}
             \PY{n}{err\PYZus{}final} \PY{o}{=} \PY{n+nb}{sum}\PY{p}{(}\PY{n}{err\PYZus{}f}\PY{p}{)}\PY{o}{/}\PY{n+nb}{len}\PY{p}{(}\PY{n}{err\PYZus{}f}\PY{p}{)}
             \PY{n}{err\PYZus{}train} \PY{o}{=} \PY{n}{dt}\PY{o}{.}\PY{n}{training\PYZus{}error}\PY{p}{(}\PY{p}{)}
             \PY{n}{m}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{i}\PY{p}{)}
             \PY{c+c1}{\PYZsh{}print( err\PYZus{}final, err\PYZus{}train)}
             \PY{n}{err\PYZus{}abs}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n+nb}{abs}\PY{p}{(}\PY{n}{err\PYZus{}final}\PY{o}{\PYZhy{}}\PY{n}{err\PYZus{}train}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
100\%|██████████████████████████████████████████████████████████████████████████████████| 98/98 [12:13<00:00, 10.12s/it]

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}15}]:} \PY{n}{sns}\PY{o}{.}\PY{n}{lineplot}\PY{p}{(}\PY{n}{m}\PY{p}{,}\PY{n}{err\PYZus{}abs}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Training Size (m)}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Marginal Error}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Marginal error vs the training size (m)}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}15}]:} Text(0.5,1,'Marginal error vs the training size (m)')
\end{Verbatim}
            
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_25_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    As, we can see the adding of additional data to training leads to
decrease in the error of decision tree. However, after a point increase
in training data does not really lead to greater decrease in marginal
error. For a feature = k, there are 2\^{}k possible X and Y events. Once
the decision tree is trained on that data, error reduces to almost nil
as generating a new test data will be a combination of original train
data.

    \subsubsection{6) Design an alternative metric for splitting the data,
not based on information content / information gain. Repeat the
computation from (5) above for your metric, and compare the performance
of your trees vs the ID3
trees}\label{design-an-alternative-metric-for-splitting-the-data-not-based-on-information-content-information-gain.-repeat-the-computation-from-5-above-for-your-metric-and-compare-the-performance-of-your-trees-vs-the-id3-trees}

    Now I build the same decision tree, but using the gini-impurity as the
metric to identify the split variable.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}1}]:} \PY{k}{class} \PY{n+nc}{Decision\PYZus{}Tree\PYZus{}Gini\PYZus{}Impurity}\PY{p}{(}\PY{p}{)}\PY{p}{:}
            \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{l+s+sd}{    Decision Tree Gini Impurity is trained on data with a target variable. }
        \PY{l+s+sd}{    It is built on split variable which is indentified using the }
        \PY{l+s+sd}{    logic of Gini Impurity.}
        \PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
            \PY{k}{def} \PY{n+nf}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{root} \PY{o}{=} \PY{k+kc}{None}\PY{p}{)}\PY{p}{:}
                \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{root} \PY{o}{=} \PY{n}{root}
                
                
            \PY{k}{def} \PY{n+nf}{gini\PYZus{}index}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{data}\PY{p}{,} \PY{n}{variable}\PY{p}{)}\PY{p}{:}
                \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{l+s+sd}{        Calcuates the gini index for the given data and target variable}
        \PY{l+s+sd}{        \PYZdq{}\PYZdq{}\PYZdq{}}
                \PY{c+c1}{\PYZsh{} Calculate the entropy of target variable}
                \PY{n}{gini\PYZus{}index} \PY{o}{=} \PY{l+m+mi}{1} \PY{o}{\PYZhy{}} \PY{n+nb}{sum}\PY{p}{(}\PY{p}{[}\PY{n+nb}{pow}\PY{p}{(}\PY{n}{data}\PY{p}{[}\PY{n}{variable}\PY{p}{]}\PY{o}{.}\PY{n}{value\PYZus{}counts}\PY{p}{(}\PY{p}{)}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{o}{/} 
                                          \PY{n}{data}\PY{p}{[}\PY{n}{variable}\PY{p}{]}\PY{o}{.}\PY{n}{count}\PY{p}{(}\PY{p}{)}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{)} 
                                      \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n}{data}\PY{p}{[}\PY{n}{variable}\PY{p}{]}\PY{o}{.}\PY{n}{unique}\PY{p}{(}\PY{p}{)}\PY{p}{]}\PY{p}{)}
                \PY{k}{return} \PY{n}{gini\PYZus{}index}
            
            
            \PY{k}{def} \PY{n+nf}{gini\PYZus{}gain}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{data}\PY{p}{,} \PY{n}{variable}\PY{p}{,} \PY{n}{target}\PY{p}{)}\PY{p}{:}
                \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{l+s+sd}{        Calculates the gini impurity for the given variable and data}
        \PY{l+s+sd}{        \PYZdq{}\PYZdq{}\PYZdq{}}
                \PY{n}{gini\PYZus{}index\PYZus{}varaible} \PY{o}{=} \PY{n+nb}{sum}\PY{p}{(}\PY{p}{[}\PY{n}{data}\PY{p}{[}\PY{n}{variable}\PY{p}{]}\PY{o}{.}\PY{n}{value\PYZus{}counts}\PY{p}{(}\PY{p}{)}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{o}{/}
                                           \PY{n}{data}\PY{p}{[}\PY{n}{variable}\PY{p}{]}\PY{o}{.}\PY{n}{count}\PY{p}{(}\PY{p}{)} \PY{o}{*} 
                                           \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{gini\PYZus{}index}\PY{p}{(}\PY{n}{data}\PY{p}{[}\PY{n}{data}\PY{p}{[}\PY{n}{variable}\PY{p}{]}\PY{o}{==} \PY{n}{i}\PY{p}{]}\PY{p}{,} 
                                                           \PY{n}{target}\PY{p}{)} \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} 
                                           \PY{n}{data}\PY{p}{[}\PY{n}{variable}\PY{p}{]}\PY{o}{.}\PY{n}{unique}\PY{p}{(}\PY{p}{)}\PY{p}{]}\PY{p}{)}
                \PY{n}{gini\PYZus{}gain\PYZus{}value} \PY{o}{=} \PY{n}{gini\PYZus{}index\PYZus{}varaible}
                \PY{k}{return}\PY{p}{(}\PY{n}{gini\PYZus{}gain\PYZus{}value}\PY{p}{)}
            
            
            \PY{k}{def} \PY{n+nf}{split\PYZus{}variable\PYZus{}identification}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{data}\PY{p}{,} \PY{n}{target}\PY{p}{)}\PY{p}{:}
                \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{l+s+sd}{        Identifies the split variable based on data and target}
        \PY{l+s+sd}{        \PYZdq{}\PYZdq{}\PYZdq{}}
                \PY{c+c1}{\PYZsh{}loop through all features and calculate gini gain for each feature}
                \PY{n}{variable\PYZus{}ig\PYZus{}required} \PY{o}{=} \PY{n+nb}{list}\PY{p}{(}\PY{n}{data}\PY{o}{.}\PY{n}{columns}\PY{p}{)}
                \PY{n}{variable\PYZus{}ig\PYZus{}required}\PY{o}{.}\PY{n}{remove}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Y}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
                \PY{n}{ig\PYZus{}values} \PY{o}{=} \PY{p}{[}\PY{p}{(}\PY{n}{i}\PY{p}{,} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{gini\PYZus{}gain}\PY{p}{(}\PY{n}{data}\PY{p}{,}\PY{n}{i}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Y}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)} \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} 
                             \PY{n}{variable\PYZus{}ig\PYZus{}required}\PY{p}{]}
                \PY{k}{if} \PY{n+nb}{len}\PY{p}{(}\PY{n}{ig\PYZus{}values}\PY{p}{)} \PY{o}{!=} \PY{l+m+mi}{0}\PY{p}{:}
                    \PY{n}{split\PYZus{}variable} \PY{o}{=} \PY{n+nb}{min}\PY{p}{(}\PY{n}{ig\PYZus{}values}\PY{p}{,} \PY{n}{key} \PY{o}{=} \PY{k}{lambda} \PY{n}{item} \PY{p}{:} \PY{n}{item}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}
                \PY{k}{else}\PY{p}{:}
                    \PY{n}{split\PYZus{}variable} \PY{o}{=} \PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{)}
                \PY{k}{return}\PY{p}{(}\PY{n}{split\PYZus{}variable}\PY{p}{)}
            
        
            \PY{k}{def} \PY{n+nf}{split\PYZus{}data}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{data}\PY{p}{,} \PY{n}{split\PYZus{}variable}\PY{p}{)}\PY{p}{:} 
                \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{l+s+sd}{        Splits the data after identifying the split variable, assigns }
        \PY{l+s+sd}{        data and rule to the node.}
        \PY{l+s+sd}{        \PYZdq{}\PYZdq{}\PYZdq{}}
                \PY{n}{splitted\PYZus{}data} \PY{o}{=} \PY{p}{[}\PY{n}{Node}\PY{p}{(}\PY{n}{data} \PY{o}{=} \PY{p}{(}\PY{n}{data}\PY{p}{[}\PY{n}{data}\PY{p}{[}\PY{n}{split\PYZus{}variable}\PY{p}{]} \PY{o}{==} \PY{n}{i}\PY{p}{]}\PY{o}{.}
                                              \PY{n}{drop}\PY{p}{(}\PY{n}{split\PYZus{}variable}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}\PY{p}{,} 
                                      \PY{n}{rule} \PY{o}{=} \PY{p}{(}\PY{n}{split\PYZus{}variable}\PY{p}{,}\PY{n}{i}\PY{p}{)}\PY{p}{)} \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} 
                                 \PY{n}{data}\PY{p}{[}\PY{n}{split\PYZus{}variable}\PY{p}{]}\PY{o}{.}\PY{n}{unique}\PY{p}{(}\PY{p}{)}\PY{p}{]}
                \PY{k}{return}\PY{p}{(}\PY{n}{splitted\PYZus{}data}\PY{p}{)}
            
            
            \PY{k}{def} \PY{n+nf}{fit}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{data}\PY{p}{,} \PY{n}{target}\PY{p}{)}\PY{p}{:}
                \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{l+s+sd}{        Fit is used to fit decision trees on the data for a given target variable}
        \PY{l+s+sd}{        \PYZdq{}\PYZdq{}\PYZdq{}}
                \PY{k}{if} \PY{n+nb}{type}\PY{p}{(}\PY{n}{data}\PY{p}{)} \PY{o}{!=} \PY{n}{Node}\PY{p}{:}
                    \PY{n}{data} \PY{o}{=} \PY{n}{Node}\PY{p}{(}\PY{n}{data} \PY{o}{=} \PY{n}{data}\PY{p}{)}
                    \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{root} \PY{o}{=} \PY{n}{data}
        
                \PY{c+c1}{\PYZsh{}if self.split\PYZus{}variable\PYZus{}identification(data.data, target)[1] == 0:}
                \PY{c+c1}{\PYZsh{}    return}
        
                \PY{n}{split\PYZus{}variable} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{split\PYZus{}variable\PYZus{}identification}\PY{p}{(}\PY{n}{data}\PY{o}{.}\PY{n}{data}\PY{p}{,}
                                                                    \PY{n}{target}\PY{p}{)}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
                \PY{n}{data}\PY{o}{.}\PY{n}{child} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{split\PYZus{}data}\PY{p}{(}\PY{n}{data}\PY{o}{.}\PY{n}{data}\PY{p}{,} \PY{n}{split\PYZus{}variable}\PY{p}{)}
        
                \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n}{data}\PY{o}{.}\PY{n}{child}\PY{p}{:}
                    \PY{k}{if} \PY{n}{i}\PY{o}{.}\PY{n}{data}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Y}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{nunique}\PY{p}{(}\PY{p}{)} \PY{o}{!=} \PY{l+m+mi}{1}\PY{p}{:} 
                        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{i}\PY{p}{,} \PY{n}{target}\PY{p}{)}
                    
            
            \PY{k}{def} \PY{n+nf}{get\PYZus{}rules}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{model} \PY{o}{=} \PY{k+kc}{None} \PY{p}{,}\PY{n}{ruleList} \PY{o}{=} \PY{p}{[}\PY{p}{]}\PY{p}{)}\PY{p}{:}
                \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{l+s+sd}{        Returns the rules for each leaf and the major class in the leaf}
        \PY{l+s+sd}{        \PYZdq{}\PYZdq{}\PYZdq{}}
                \PY{k}{if} \PY{n}{model} \PY{o}{==} \PY{k+kc}{None}\PY{p}{:}
                    \PY{n}{model} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{root}
                \PY{n}{ruleList}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{model}\PY{o}{.}\PY{n}{rule}\PY{p}{)}
        
                \PY{k}{if} \PY{n}{model}\PY{o}{.}\PY{n}{child} \PY{o}{==} \PY{k+kc}{None}\PY{p}{:}
                    \PY{n}{ruleList}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{model}\PY{o}{.}\PY{n}{data}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Y}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{mode}\PY{p}{(}\PY{p}{)}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}
                    \PY{k}{return} \PY{n+nb}{print}\PY{p}{(}\PY{n}{ruleList}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{:}\PY{p}{]}\PY{p}{)}
        
                \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n}{model}\PY{o}{.}\PY{n}{child}\PY{p}{:}
                    \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{get\PYZus{}rules}\PY{p}{(}\PY{n}{i}\PY{p}{,}\PY{n}{ruleList}\PY{o}{.}\PY{n}{copy}\PY{p}{(}\PY{p}{)}\PY{p}{)}
            
        
            \PY{k}{def} \PY{n+nf}{predict\PYZus{}row}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{model}\PY{p}{,} \PY{n}{row}\PY{p}{)}\PY{p}{:}
                \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{l+s+sd}{        This function returns the prediction for the a single sample of }
        \PY{l+s+sd}{        data using the fitted data}
        \PY{l+s+sd}{        \PYZdq{}\PYZdq{}\PYZdq{}}
                \PY{k}{if} \PY{n}{model}\PY{o}{.}\PY{n}{child} \PY{o}{==} \PY{k+kc}{None}\PY{p}{:}
                    \PY{k}{return}\PY{p}{(}\PY{n}{model}\PY{o}{.}\PY{n}{data}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Y}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{mode}\PY{p}{(}\PY{p}{)}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}
        
                \PY{n}{variable} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{child}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{rule}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
                \PY{n}{row\PYZus{}value} \PY{o}{=} \PY{n}{row}\PY{p}{[}\PY{n}{variable}\PY{p}{]}
                \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n}{model}\PY{o}{.}\PY{n}{child}\PY{p}{:}
                    \PY{k}{if} \PY{n}{i}\PY{o}{.}\PY{n}{rule}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]} \PY{o}{==} \PY{n}{row\PYZus{}value}\PY{p}{:}
                        \PY{k}{return} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{predict\PYZus{}row}\PY{p}{(}\PY{n}{i}\PY{p}{,} \PY{n}{row}\PY{p}{)}
                    
            \PY{k}{def} \PY{n+nf}{predict}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{test}\PY{p}{)}\PY{p}{:}
                \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{l+s+sd}{        Predict funtion will take an input data and return the prediction }
        \PY{l+s+sd}{        based on the fitted decision tree}
        \PY{l+s+sd}{        \PYZdq{}\PYZdq{}\PYZdq{}}
                \PY{n}{predicted\PYZus{}y} \PY{o}{=} \PY{p}{[}\PY{p}{]}
                \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n}{test}\PY{o}{.}\PY{n}{iterrows}\PY{p}{(}\PY{p}{)}\PY{p}{:}
                    \PY{n}{x} \PY{o}{=} \PY{n}{i}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}
                    \PY{n}{y} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{predict\PYZus{}row}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{root}\PY{p}{,} \PY{n}{x}\PY{p}{)}
                    \PY{n}{predicted\PYZus{}y}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{y}\PY{p}{)}
                \PY{k}{return} \PY{n}{pd}\PY{o}{.}\PY{n}{Series}\PY{p}{(}\PY{n}{predicted\PYZus{}y}\PY{p}{)}
            
            \PY{k}{def} \PY{n+nf}{training\PYZus{}error}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{)}\PY{p}{:}
                \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{l+s+sd}{        Returns the training error of the  fitted decision tree}
        \PY{l+s+sd}{        \PYZdq{}\PYZdq{}\PYZdq{}}
                \PY{n}{predict\PYZus{}train} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{data}\PY{p}{)}
                \PY{k}{return} \PY{p}{(}\PY{l+m+mi}{1} \PY{o}{\PYZhy{}}\PY{n+nb}{sum}\PY{p}{(}\PY{n}{data}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Y}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{==} \PY{n}{predict\PYZus{}train}\PY{p}{)}\PY{o}{/} \PY{n+nb}{len}\PY{p}{(}\PY{n}{data}\PY{p}{)}\PY{p}{)}
                    
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}7}]:} \PY{n}{data} \PY{o}{=} \PY{n}{data\PYZus{}generator}\PY{p}{(}\PY{l+m+mi}{30}\PY{p}{,}\PY{l+m+mi}{4}\PY{p}{)}
        \PY{n}{model} \PY{o}{=} \PY{n}{Decision\PYZus{}Tree\PYZus{}Gini\PYZus{}Impurity}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}8}]:} \PY{n}{model}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{data}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Y}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}9}]:} \PY{n}{model}\PY{o}{.}\PY{n}{get\PYZus{}rules}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
[('X1', 0), ('X3', 0), 1]
[('X1', 0), ('X3', 1), ('X4', 1), 0]
[('X1', 0), ('X3', 1), ('X4', 0), 1]
[('X1', 1), ('X3', 1), 1]
[('X1', 1), ('X3', 0), ('X2', 0), 0]
[('X1', 1), ('X3', 0), ('X2', 1), ('X4', 0), 0]
[('X1', 1), ('X3', 0), ('X2', 1), ('X4', 1), 1]

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}10}]:} \PY{n}{model}\PY{o}{.}\PY{n}{training\PYZus{}error}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}10}]:} 0.0
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}11}]:} \PY{c+c1}{\PYZsh{}Let us define the values required to understand the marginal value of additional }
         \PY{c+c1}{\PYZsh{}training data}
         \PY{n}{k} \PY{o}{=} \PY{l+m+mi}{10}
         
         \PY{c+c1}{\PYZsh{}Defining the repetitions values for generating m samples of training data}
         \PY{n}{start} \PY{o}{=} \PY{l+m+mi}{50}
         \PY{n}{end} \PY{o}{=} \PY{l+m+mi}{2500}
         \PY{n}{step} \PY{o}{=} \PY{l+m+mi}{25}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}14}]:} \PY{n}{err\PYZus{}abs2} \PY{o}{=} \PY{p}{[}\PY{p}{]}
         \PY{n}{m2} \PY{o}{=} \PY{p}{[}\PY{p}{]}
         \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n}{tqdm}\PY{p}{(}\PY{n+nb}{range}\PY{p}{(}\PY{n}{start}\PY{p}{,}\PY{n}{end}\PY{p}{,}\PY{n}{step}\PY{p}{)}\PY{p}{)}\PY{p}{:}
             \PY{n}{data} \PY{o}{=} \PY{n}{data\PYZus{}generator}\PY{p}{(}\PY{n}{i}\PY{p}{,} \PY{n}{k} \PY{o}{=} \PY{n}{k}\PY{p}{)}
             \PY{n}{dt} \PY{o}{=} \PY{n}{Decision\PYZus{}Tree\PYZus{}Gini\PYZus{}Impurity}\PY{p}{(}\PY{p}{)}
             \PY{n}{dt}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{data}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Y}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
             \PY{n}{err\PYZus{}f} \PY{o}{=} \PY{n}{typical\PYZus{}error}\PY{p}{(}\PY{n}{dt}\PY{p}{,} \PY{l+m+mi}{100}\PY{p}{)}
             \PY{n}{err\PYZus{}final} \PY{o}{=} \PY{n+nb}{sum}\PY{p}{(}\PY{n}{err\PYZus{}f}\PY{p}{)}\PY{o}{/}\PY{n+nb}{len}\PY{p}{(}\PY{n}{err\PYZus{}f}\PY{p}{)}
             \PY{n}{err\PYZus{}train} \PY{o}{=} \PY{n}{dt}\PY{o}{.}\PY{n}{training\PYZus{}error}\PY{p}{(}\PY{p}{)}
             \PY{n}{m2}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{i}\PY{p}{)}
             \PY{c+c1}{\PYZsh{}print( err\PYZus{}final, err\PYZus{}train)}
             \PY{n}{err\PYZus{}abs2}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n+nb}{abs}\PY{p}{(}\PY{n}{err\PYZus{}final}\PY{o}{\PYZhy{}}\PY{n}{err\PYZus{}train}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
100\%|██████████████████████████████████████████████████████████████████████████████████| 98/98 [05:34<00:00,  4.24s/it]

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}15}]:} \PY{n}{sns}\PY{o}{.}\PY{n}{lineplot}\PY{p}{(}\PY{n}{m2}\PY{p}{,}\PY{n}{err\PYZus{}abs2}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Training Size (m)}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Marginal Error}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Marginal error vs the training size (m)}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}15}]:} Text(0.5,1,'Marginal error vs the training size (m)')
\end{Verbatim}
            
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_36_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    We see a similar trend here as compared to the ID3 decision tree. Both
of the methods of splitting seems to produce a close marginal error. On
increasing the training data size, the marginal error reduces here same
as in the case of ID3 decision tree.


    % Add a bibliography block to the postdoc
    
    
    
    \end{document}
